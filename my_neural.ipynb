{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークで学習してみるファイル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import utils\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42) # 乱数生成シード\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Grab a GPU if there is one\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using {} device: {}\".format(device, torch.cuda.current_device()))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(196072, 3, 500)\n",
      "(196072,)\n"
     ]
    }
   ],
   "source": [
    "fold = \"../train_raw_npy/\"\n",
    "xyz = np.load(f\"{fold}acc_xyz.npy\")\n",
    "label = np.load(f\"{fold}sampled_label.npy\")\n",
    "print(xyz.shape)\n",
    "print(label.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train / test spilit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170000, 3, 500)\n",
      "(20000, 3, 500)\n"
     ]
    }
   ],
   "source": [
    "pin = 170000\n",
    "train_xyz = xyz[0:pin,:,:]\n",
    "test_xyz = xyz[pin:190000, :, :]\n",
    "train_label = label[0:pin]\n",
    "test_label = label[pin:190000]\n",
    "print(train_xyz.shape)\n",
    "print(test_xyz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0    27641\n",
      "5.0    26056\n",
      "6.0    25689\n",
      "8.0    22016\n",
      "2.0    21185\n",
      "1.0    20712\n",
      "4.0    19603\n",
      "3.0     7098\n",
      "Name: count, dtype: int64\n",
      "7.0    3612\n",
      "1.0    3315\n",
      "4.0    2766\n",
      "6.0    2637\n",
      "2.0    2489\n",
      "5.0    2419\n",
      "8.0    1818\n",
      "3.0     944\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(train_label).value_counts())\n",
    "print(pd.Series(test_label).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainをランダムに20000個サンプリングする。\n",
    "sample_size = 20000\n",
    "indices = np.random.choice(train_xyz.shape[0], sample_size, replace=False)\n",
    "train_xyz_random = train_xyz[indices]\n",
    "train_label_random = train_label[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 3, 500)\n"
     ]
    }
   ],
   "source": [
    "print(train_xyz_random.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# アーキテクチャデザイン  \n",
    "オックスフォードでの例にあるようにCNNを使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBNReLU(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            bias=True\n",
    "    ):\n",
    "        super(ConvBNReLU, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, bias=bias),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.main(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, output_size=8, in_channels=3, num_filters_init=8):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            ConvBNReLU(in_channels, num_filters_init, 8,2,3), # 500 -> 250 \n",
    "            ConvBNReLU(num_filters_init, num_filters_init*2, 6,2,2 ), # 250 -> 125 (248)\n",
    "            ConvBNReLU(num_filters_init*2, num_filters_init*4,7,2,2 ), # 125 -> 62\n",
    "            ConvBNReLU(num_filters_init*4, num_filters_init*8,6,2,1 ), # 62 -> 30\n",
    "            ConvBNReLU(num_filters_init*8, num_filters_init*16, 4, 4, 1), # 30 -> 8\n",
    "            ConvBNReLU(num_filters_init*16, num_filters_init*32, 8,1,0), # 8 -> 1\n",
    "            nn.Conv1d(num_filters_init*32, output_size, 1,1,0)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.cnn(x).view(x.shape[0], -1) # ここってもしかして(N, 1500)にしていたりするのかな？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "懸念事項１  \n",
    "cnnのカーネルとかのサイズの計算が合わない可能性がある"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "懸念事項３  \n",
    "クラスラベルの変更0~7に  \n",
    "これは関数側で対応することにする。  \n",
    "学習時や推論時は考えなくていいようにする。(SHL用の関数を作成するということ)  \n",
    "0~7クラス分類で学習し、1~8クラス分類として表示する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(X, y=None, batch_size=1, shuffle=False):\n",
    "    if shuffle:\n",
    "        idxs = np.random.permutation(np.arange(len(X)))\n",
    "    else:\n",
    "        idxs = np.arange(len(X))\n",
    "    \n",
    "    #データセットをバッチサイズごとに分割\n",
    "    for i in range(0, len(idxs), batch_size):\n",
    "        idxs_batch = idxs[i:i+batch_size]\n",
    "        X_batch = X[idxs_batch].astype('f4')\n",
    "\n",
    "        X_batch = torch.from_numpy(X_batch)\n",
    "        if y is None:\n",
    "            yield X_batch\n",
    "        else:\n",
    "            y_batch = y[idxs_batch]\n",
    "            y_batch = torch.from_numpy(y_batch-1)\n",
    "            yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_by_batches(cnn, X):\n",
    "    Y = []\n",
    "    cnn.eval()\n",
    "    with torch.no_grad():\n",
    "        for x in create_dataloader(X, batch_size=1024, shuffle=False):\n",
    "            x = x.to(device)\n",
    "            Y.append(cnn(x))\n",
    "    cnn.train()\n",
    "\n",
    "    Y = torch.cat(Y) # Yをテンソルに変換\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(cnn, X, Y):\n",
    "    Y_pred = forward_by_batches(cnn, X)\n",
    "    loss = F.cross_entropy(Y_pred, torch.from_numpy(Y-1.0).type(torch.int64).to(device)).item() # クロスエントロピー損失の計算 仕方なく0~7クラス分類とする\n",
    "\n",
    "    Y_pred = F.softmax(Y_pred, dim=1) \n",
    "    Y_pred = torch.argmax(Y_pred, dim=1)  # 最も高い確率のY_predのラベルを予測ラベルとしてY_predに入れられる。\n",
    "    Y_pred = Y_pred + 1  # 予測ラベルに1を加えて1~8の範囲に変換する\n",
    "    Y_pred = Y_pred.cpu().numpy()  # テンソルでGPUにのっているものをcpuに移動して、それをさらにnumpy配列に変換している。\n",
    "    kappa = metrics.cohen_kappa_score(Y, Y_pred) # 1~8クラス分類\n",
    "\n",
    "    return {'loss':loss, 'kappa':kappa, 'Y_pred':Y_pred}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "懸念事項２  \n",
    "クラス分類は1~8である。したがって0~7のクラス分類を1~8に移動させてあげる必要がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filters_init = 32 \n",
    "in_channels = 3  \n",
    "output_size = len(np.unique(Y))  #　これは分類するラベルの数。最終層の特徴量の数\n",
    "num_epoch = 5  # num of epochs (full loops though the training set)\n",
    "lr = 3e-4  # learning rate\n",
    "batch_size = 32  # size of the mini-batch　バッチサイズを32にしている。\n",
    "\n",
    "cnn = CNN(\n",
    "    output_size=output_size,\n",
    "    in_channels=in_channels,\n",
    "    num_filters_init=num_filters_init\n",
    ").to(device)\n",
    "print(cnn) # ここではcnnのネットワーク構造の定義をして、そのネットワークの構造をprintしている。\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # クロスエントロピー損失を使用してモデルの出力と実際のラベルとの差を計算するためのインスタンス定義\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=lr) # オプティマイザーを使用し、設定された学習率lrでモデルのパラメータを最適化する。Adamはよく使われるやつでパフォーマンスが高いらしい　\n",
    "# optim.Adam()ではcnnのパラメータへの直接参照をすることができるnum_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_history_test = []\n",
    "loss_history_test = []\n",
    "loss_history_train = []\n",
    "losses = []\n",
    "num_epoch = 10\n",
    "for i in tqdm(range(num_epoch)): # 同じデータセットでnum_epoch回cnnを学習させている。\n",
    "    dataloader = create_dataloader(X_train, Y_train, batch_size, shuffle=True)\n",
    "    for x, target in dataloader:\n",
    "        x, target = x.to(device), target.type(torch.int64).to(device)\n",
    "        cnn.zero_grad()\n",
    "        output = cnn(x) # バッチxをネットワークに通し、予測値outputを取得\n",
    "        loss = loss_fn(output, target) # 予測値と実際のラベルtargetとの間の損失を計算する。ネットワークの出力outputと正解targetとの間の差異を計算しその結果としてスカラー値lossを生成する。\n",
    "        loss.backward() # 計算されたlossに従ってネットワークのすべてのパラメータに対する勾配が計算される。\n",
    "        optimizer.step() # オプティマイザが保存している勾配情報を使用してモデルのパラメータを更新　直接cnnに紐づいているのでcnnのパラメータを直接更新している\n",
    "\n",
    "        # Logging -- track train loss\n",
    "        losses.append(loss.item()) # ログを残している。lossというのはパラメータのgradientでパラメータを更新する方向や大きさを決めている。\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    #       Evaluate performance at the end of each epoch\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    # Logging -- average train loss in this epoch\n",
    "    loss_history_train.append(utils.ewm(losses))\n",
    "\n",
    "    # Logging -- evalutate performance on test set\n",
    "    results = evaluate_model(cnn, X_test, Y_test)\n",
    "    loss_history_test.append(results['loss'])\n",
    "    kappa_history_test.append(results['kappa'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labo_Okita",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
