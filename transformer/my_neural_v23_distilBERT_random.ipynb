{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここではtrainデータ全体からランダムにデータを取る。  \n",
    "データ数を増やすことで性能向上することを示すには全体の訓練データからランダムにデータを取ってくる必要があると考えた。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertModel, BertConfig\n",
    "import seaborn as sns\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import utils\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42) # 乱数生成シード\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Grab a GPU if there is one\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using {} device: {}\".format(device, torch.cuda.current_device()))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(196072, 500)\n",
      "(196072,)\n"
     ]
    }
   ],
   "source": [
    "fold = \"../../train_raw_npy/\"\n",
    "acc_x = np.loadtxt(\"../../train_raw/Acc_x.txt\")\n",
    "label = np.load(f\"{fold}sampled_label.npy\")\n",
    "\n",
    "print(acc_x.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max index after shifting: 30104\n",
      "Vocab size: 30105\n",
      "Sample discrete data: [[24029 24034 24075 ...  6255  7731  8392]\n",
      " [ 8839  8946  7772 ...  9095  9134  8994]\n",
      " [ 8474  7847  7012 ... 25182 25115 25278]\n",
      " ...\n",
      " [16043 14565 19002 ... 26111 21273  9221]\n",
      " [ 3849 12370 16115 ... 23046 24751 23873]\n",
      " [12795 12362 21437 ...  7500 12527 17853]]\n",
      "Discrete data range: 104 to 30104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'# 分布の視覚化\\nplt.figure(figsize=(12, 6))\\nsns.histplot(discrete_data.flatten(), bins=num_bins, kde=False)\\nplt.title(f\"Distribution of Discretized Accelerometer X-axis Data ({num_bins} bins, equal-frequency)\")\\nplt.xlabel(\"Discrete Value\")\\nplt.ylabel(\"Frequency\")\\nplt.show()'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ビンの数を設定\n",
    "num_bins = 30000  \n",
    "\n",
    "# ビンの境界を等頻度で設定\n",
    "bins = np.percentile(acc_x, np.linspace(0, 100, num_bins + 1))\n",
    "\n",
    "# データをビンに分割\n",
    "discrete_data = np.digitize(acc_x, bins) - 1  # ビンのインデックスを取得し、0から始まるように調整\n",
    "\n",
    "# 予約トークンを避けるためにシフト\n",
    "discrete_data += 104\n",
    "\n",
    "# 最大インデックスを確認\n",
    "max_index = discrete_data.max()\n",
    "print(f\"Max index after shifting: {max_index}\")\n",
    "\n",
    "# vocab_sizeを確認（シフト後の最大値を考慮）\n",
    "vocab_size = max_index + 1\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "\n",
    "# 確認のための一部データ\n",
    "print(\"Sample discrete data:\", discrete_data[:10])\n",
    "print(\"Discrete data range:\", np.min(discrete_data), \"to\", np.max(discrete_data))\n",
    "\n",
    "\"\"\"# 分布の視覚化\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(discrete_data.flatten(), bins=num_bins, kde=False)\n",
    "plt.title(f\"Distribution of Discretized Accelerometer X-axis Data ({num_bins} bins, equal-frequency)\")\n",
    "plt.xlabel(\"Discrete Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\"\"\"\n",
    "\n",
    "# 離散化されたデータを保存\n",
    "#np.save(\"train_token_ids_rebinned.npy\", discrete_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196072, 500)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discrete_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 500)\n",
      "(30000,)\n"
     ]
    }
   ],
   "source": [
    "#説明変数\n",
    "discrete_data\n",
    "#目的変数\n",
    "label\n",
    "\n",
    "# ランダム３万データを使う  \n",
    "size = 30000\n",
    "sample_indices = np.random.choice(discrete_data.shape[0], size=size, replace=False)\n",
    "X_30000 = discrete_data[sample_indices, :]\n",
    "label_30000 = label[sample_indices]\n",
    "print(X_30000.shape)\n",
    "print(label_30000.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n",
      "float64\n",
      "int32\n",
      "int32\n"
     ]
    }
   ],
   "source": [
    "print(X_30000.dtype)\n",
    "print(label_30000.dtype)\n",
    "X_30000 =X_30000.astype(np.int32)\n",
    "label_30000 = label_30000.astype(np.int32)\n",
    "print(X_30000.dtype)\n",
    "print(label_30000.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (24000, 500)\n",
      "Test data shape: (6000, 500)\n",
      "Train label shape: (24000,)\n",
      "Test label shape: (6000,)\n",
      "5    3907\n",
      "7    3907\n",
      "6    3490\n",
      "1    3023\n",
      "2    2969\n",
      "8    2935\n",
      "4    2804\n",
      "3     965\n",
      "Name: count, dtype: int64\n",
      "5    1026\n",
      "7     972\n",
      "6     853\n",
      "2     765\n",
      "4     725\n",
      "1     708\n",
      "8     706\n",
      "3     245\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ランダムサンプリング\n",
    "train_x, test_x, train_label, test_label = train_test_split(X_30000, label_30000, test_size=0.2,random_state=40)\n",
    "\n",
    "# 結果の確認\n",
    "print(f\"Train data shape: {train_x.shape}\")\n",
    "print(f\"Test data shape: {test_x.shape}\")\n",
    "print(f\"Train label shape: {train_label.shape}\")\n",
    "print(f\"Test label shape: {test_label.shape}\")\n",
    "print(pd.Series(train_label).value_counts())\n",
    "print(pd.Series(test_label).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SensorDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.data[idx], dtype=torch.long)\n",
    "        labels = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        # [CLS] と [SEP] トークンを追加\n",
    "        input_ids = torch.cat([torch.tensor([101]), input_ids, torch.tensor([102])])\n",
    "\n",
    "        attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 5 6 ... 2 5 5]\n",
      "[5 4 5 ... 1 4 4]\n",
      "[5 5 5 ... 2 7 5]\n",
      "[4 4 4 ... 1 6 4]\n"
     ]
    }
   ],
   "source": [
    "# ラベルを0~7クラス分類とする\n",
    "print(train_label)\n",
    "train_label -=1\n",
    "print(train_label)\n",
    "print(test_label)\n",
    "test_label -=1\n",
    "print(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットとデータローダの作成\n",
    "train_dataset = SensorDataset(train_x, train_label)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "eval_dataset = SensorDataset(test_x, test_label)\n",
    "eval_dataloader = DataLoader(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30105, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=8, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#DistilBERT\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertConfig\n",
    "\n",
    "config = DistilBertConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    num_labels=8,\n",
    ")\n",
    "model = DistilBertForSequenceClassification(config)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kaiha\\anaconda3\\envs\\labo_Okita\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6aa5a19ce7426b87706aa0c007f534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0721, 'grad_norm': 3.646036148071289, 'learning_rate': 4.888888888888889e-05, 'epoch': 0.07}\n",
      "{'loss': 2.0535, 'grad_norm': 4.366201400756836, 'learning_rate': 4.7777777777777784e-05, 'epoch': 0.13}\n",
      "{'loss': 2.0222, 'grad_norm': 2.8405861854553223, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.2}\n",
      "{'loss': 1.9047, 'grad_norm': 4.834967613220215, 'learning_rate': 4.555555555555556e-05, 'epoch': 0.27}\n",
      "{'loss': 1.7901, 'grad_norm': 5.8292036056518555, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}\n",
      "{'loss': 1.7307, 'grad_norm': 6.301307678222656, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}\n",
      "{'loss': 1.6618, 'grad_norm': 6.74536657333374, 'learning_rate': 4.222222222222222e-05, 'epoch': 0.47}\n",
      "{'loss': 1.6556, 'grad_norm': 4.097332954406738, 'learning_rate': 4.111111111111111e-05, 'epoch': 0.53}\n",
      "{'loss': 1.5643, 'grad_norm': 5.949358940124512, 'learning_rate': 4e-05, 'epoch': 0.6}\n",
      "{'loss': 1.6082, 'grad_norm': 6.374051094055176, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52936988c06f49ddb0d5c6e30f9b302a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5308200120925903, 'eval_runtime': 27.2628, 'eval_samples_per_second': 220.08, 'eval_steps_per_second': 27.51, 'epoch': 0.67}\n",
      "{'loss': 1.5946, 'grad_norm': 8.300867080688477, 'learning_rate': 3.777777777777778e-05, 'epoch': 0.73}\n",
      "{'loss': 1.5509, 'grad_norm': 6.908094882965088, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}\n",
      "{'loss': 1.5764, 'grad_norm': 5.093883514404297, 'learning_rate': 3.555555555555556e-05, 'epoch': 0.87}\n",
      "{'loss': 1.5281, 'grad_norm': 4.997161865234375, 'learning_rate': 3.444444444444445e-05, 'epoch': 0.93}\n",
      "{'loss': 1.5518, 'grad_norm': 5.65903377532959, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n",
      "{'loss': 1.3325, 'grad_norm': 6.216193199157715, 'learning_rate': 3.222222222222223e-05, 'epoch': 1.07}\n",
      "{'loss': 1.4049, 'grad_norm': 4.97233247756958, 'learning_rate': 3.111111111111111e-05, 'epoch': 1.13}\n",
      "{'loss': 1.3537, 'grad_norm': 9.118179321289062, 'learning_rate': 3e-05, 'epoch': 1.2}\n",
      "{'loss': 1.4196, 'grad_norm': 6.851590156555176, 'learning_rate': 2.8888888888888888e-05, 'epoch': 1.27}\n",
      "{'loss': 1.3872, 'grad_norm': 6.615692615509033, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d57d295fdfe4ef9887853e28219fd25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.495347023010254, 'eval_runtime': 26.6821, 'eval_samples_per_second': 224.87, 'eval_steps_per_second': 28.109, 'epoch': 1.33}\n"
     ]
    }
   ],
   "source": [
    "# トレーニング設定\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_distilBERT\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=0,\n",
    "    save_total_limit=None,\n",
    "    #logging_dir = \"./logs_distilBERT\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1_000,\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "# トレーナーの定義\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# トレーニングの実行\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del train_dataset\n",
    "del trainer\n",
    "del eval_dataset\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 評価  \n",
    "学習のデータ数だけ変更して性能比較"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 語彙数30105  バッチサイズ16 あとデフォルト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "final_evaluation = trainer.evaluate()\n",
    "\n",
    "# ロスの履歴を取得\n",
    "train_loss = []\n",
    "train_steps = []\n",
    "eval_loss = []\n",
    "eval_steps = []\n",
    "for log in trainer.state.log_history:\n",
    "    if \"loss\" in log.keys():\n",
    "        train_loss.append(log[\"loss\"])\n",
    "        train_steps.append(log[\"step\"])\n",
    "    if \"eval_loss\" in log.keys():\n",
    "        eval_loss.append(log[\"eval_loss\"])\n",
    "        eval_steps.append(log[\"step\"])\n",
    "\n",
    "# 最後の評価結果を追加\n",
    "eval_loss.append(final_evaluation[\"eval_loss\"])\n",
    "eval_steps.append(trainer.state.global_step)\n",
    "\n",
    "# ロスのプロット\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_steps, train_loss, label='Train Loss', color='C0')\n",
    "plt.plot(eval_steps, eval_loss, label='Eval Loss', color='C1')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Evaluation Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "predictions = trainer.predict(eval_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "#true_labels = eval_dataset[\"labels\"]\n",
    "# 分類レポートの生成\n",
    "report = classification_report(test_label, preds, target_names=[str(i) for i in range(1, 9)])\n",
    "print(report)\n",
    "\n",
    "# 混同行列の計算\n",
    "cm = confusion_matrix(test_label, preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[str(i) for i in range(1,9)], yticklabels=[str(i) for i in range(1,9)])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labo_Okita",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
